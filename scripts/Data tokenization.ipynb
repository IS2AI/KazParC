{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GPU\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import concatenate_datasets, Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for text vectorization (f.e. KK->RU)\n",
    "\n",
    "def preprocessing(examples):\n",
    "    inputs = examples[\"source_lang\"]\n",
    "    targets = examples[\"target_lang\"]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, return_tensors=\"pt\", truncation=True, \n",
    "                             padding=True, max_length=256).to('cuda')\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for text vectorization for the same data, but in the opposite direction (f.e. RU->KK)\n",
    "\n",
    "def preprocessing_inverce(examples):\n",
    "    inputs = examples[\"target_lang\"]\n",
    "    targets = examples[\"source_lang\"]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, return_tensors=\"pt\", truncation=True, \n",
    "                             padding=True, max_length=256).to('cuda')\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load synthetic EN-TR data\n",
    "\n",
    "en_tr_train_synt = pd.read_csv('./data/18_sync_train_en_tr.csv')\n",
    "en_tr_dev_synt = pd.read_csv('./data/24_sync_valid_en_tr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load KazParC EN-TR data\n",
    "\n",
    "en_tr_train_kazparc =  pd.read_csv('./data/04_kazparc_train_en_tr.csv')\n",
    "en_tr_dev_kazparc =  pd.read_csv('./data/10_kazparc_valid_en_tr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "\n",
    "en_tr_train_kazparc.rename(columns={'en': 'source_lang', 'tr': 'target_lang'}, inplace=True)\n",
    "en_tr_dev_kazparc.rename(columns={'en': 'source_lang', 'tr': 'target_lang'}, inplace=True)\n",
    "en_tr_train_synt.rename(columns={'en': 'source_lang', 'tr': 'target_lang'}, inplace=True)\n",
    "en_tr_dev_synt.rename(columns={'en': 'source_lang', 'tr': 'target_lang'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unused in training columns to reduse the datasets' size\n",
    "\n",
    "en_tr_train_synt.drop(columns=['id'], inplace=True)\n",
    "en_tr_dev_synt.drop(columns=['id'], inplace=True)\n",
    "en_tr_train_kazparc.drop(columns=['id', 'domain'], inplace=True)\n",
    "en_tr_dev_kazparc.drop(columns=['id', 'domain'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pandas datasets to HF format\n",
    "\n",
    "en_tr_train_synt = Dataset.from_pandas(en_tr_train_synt)\n",
    "en_tr_dev_synt = Dataset.from_pandas(en_tr_dev_synt)\n",
    "en_tr_train_kazparc = Dataset.from_pandas(en_tr_train_kazparc)\n",
    "en_tr_dev_kazparc = Dataset.from_pandas(en_tr_dev_kazparc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the synthetic and KazParC datasets\n",
    "\n",
    "en_tr_train_all = concatenate_datasets([en_tr_train_synt, en_tr_train_kazparc])\n",
    "en_tr_dev_all = concatenate_datasets([en_tr_dev_synt, en_tr_dev_kazparc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the tokenizer for EN->TR\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/nllb-200-distilled-1.3B', src_lang='eng_Latn', tgt_lang='tur_Latn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization for English to Turkish\n",
    "\n",
    "en_tr_tokenized_data_train = en_tr_train_all.map(preprocessing, batched=True)\n",
    "en_tr_tokenized_data_dev = en_tr_dev_all.map(preprocessing, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the tokenizer for TR->EN\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/nllb-200-distilled-1.3B', src_lang='tur_Latn', tgt_lang='eng_Latn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization for Turkish to English\n",
    "\n",
    "en_tr_tokenized_data_train2 = en_tr_train_all.map(preprocessing_inverce, batched=True)\n",
    "en_tr_tokenized_data_dev2 = en_tr_dev_all.map(preprocessing_inverce, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the EN->TR and TR->EN datasets\n",
    "\n",
    "dataset_train = concatenate_datasets([en_tr_tokenized_data_train, en_tr_tokenized_data_train2])\n",
    "dataset_dev = concatenate_datasets([en_tr_tokenized_data_dev, en_tr_tokenized_data_dev2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synthetic EN-RU data\n",
    "\n",
    "en_ru_train_synt = pd.read_csv('./data/17_sync_train_en_ru.csv')\n",
    "en_ru_dev_synt = pd.read_csv('./data/23_sync_valid_en_ru.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load KazParC EN-RU data\n",
    "\n",
    "en_ru_train_kazparc = pd.read_csv('./data/03_kazparc_train_en_ru.csv')\n",
    "en_ru_dev_kazparc = pd.read_csv('./data/09_kazparc_valid_en_ru.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "\n",
    "en_ru_train_kazparc.rename(columns={'en': 'source_lang', 'ru': 'target_lang'}, inplace=True)\n",
    "en_ru_dev_kazparc.rename(columns={'en': 'source_lang', 'ru': 'target_lang'}, inplace=True)\n",
    "en_ru_train_synt.rename(columns={'en': 'source_lang', 'ru': 'target_lang'}, inplace=True)\n",
    "en_ru_dev_synt.rename(columns={'en': 'source_lang', 'ru': 'target_lang'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unused in training columns to reduse the datasets' size\n",
    "\n",
    "en_ru_train_synt.drop(columns=['id'], inplace=True)\n",
    "en_ru_dev_synt.drop(columns=['id'], inplace=True)\n",
    "en_ru_train_kazparc.drop(columns=['id', 'domain'], inplace=True)\n",
    "en_ru_dev_kazparc.drop(columns=['id', 'domain'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pandas datasets to HF format\n",
    "\n",
    "en_ru_train_synt = Dataset.from_pandas(en_ru_train_synt)\n",
    "en_ru_dev_synt = Dataset.from_pandas(en_ru_dev_synt)\n",
    "en_ru_train_kazparc = Dataset.from_pandas(en_ru_train_kazparc)\n",
    "en_ru_dev_kazparc = Dataset.from_pandas(en_ru_dev_kazparc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the synthetic and KazParC datasets\n",
    "\n",
    "en_ru_train_all = concatenate_datasets([en_ru_train_synt, en_ru_train_kazparc])\n",
    "en_ru_dev_all = concatenate_datasets([en_ru_dev_synt, en_ru_dev_kazparc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the tokenizer for EN->RU\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-1.3B\", src_lang='eng_Latn', tgt_lang='rus_Cyrl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization for English to Russian\n",
    "\n",
    "en_ru_tokenized_data_train = en_ru_train_all.map(preprocessing, batched=True)\n",
    "en_ru_tokenized_data_dev = en_ru_dev_all.map(preprocessing, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the EN->RU and the vectorized datasets\n",
    "\n",
    "dataset_dev = concatenate_datasets([en_ru_tokenized_data_dev, dataset_dev])\n",
    "dataset_train = concatenate_datasets([en_ru_tokenized_data_train, dataset_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the tokenizer for RU->EN\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-1.3B\", src_lang='rus_Cyrl', tgt_lang='eng_Latn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization for Russian to English\n",
    "\n",
    "en_ru_tokenized_data_train2 = en_ru_train_all.map(preprocessing_inverce, batched=True)\n",
    "en_ru_tokenized_data_dev2 = en_ru_dev_all.map(preprocessing_inverce, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the RU->EN and the vectorized datasets\n",
    "\n",
    "dataset_train = concatenate_datasets([en_ru_tokenized_data_train2, dataset_train])\n",
    "dataset_dev = concatenate_datasets([en_ru_tokenized_data_dev2, dataset_dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synthetic EN-KK data\n",
    "\n",
    "kk_en_train_synt = pd.read_csv('./data/16_sync_train_en_kk.csv')\n",
    "kk_en_dev_synt = pd.read_csv('./data/22_sync_valid_en_kk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load KazParC EN-KK data\n",
    "\n",
    "kk_en_train_kazparc = pd.read_csv('./data/02_kazparc_train_en_kk.csv')\n",
    "kk_en_dev_kazparc = pd.read_csv('./data/08_kazparc_valid_en_kk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "\n",
    "kk_en_train_kazparc.rename(columns={'kk': 'source_lang', 'en': 'target_lang'}, inplace=True)\n",
    "kk_en_dev_kazparc.rename(columns={'kk': 'source_lang', 'en': 'target_lang'}, inplace=True)\n",
    "kk_en_train_synt.rename(columns={'kk': 'source_lang', 'en': 'target_lang'}, inplace=True)\n",
    "kk_en_dev_synt.rename(columns={'kk': 'source_lang', 'en': 'target_lang'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unused in training columns to reduse the datasets' size\n",
    "\n",
    "kk_en_train_synt.drop(columns=['id'], inplace=True)\n",
    "kk_en_dev_synt.drop(columns=['id'], inplace=True)\n",
    "kk_en_train_kazparc.drop(columns=['id', 'domain'], inplace=True)\n",
    "kk_en_dev_kazparc.drop(columns=['id', 'domain'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pandas datasets to HF format\n",
    "\n",
    "kk_en_train_synt = Dataset.from_pandas(kk_en_train_synt)\n",
    "kk_en_dev_synt = Dataset.from_pandas(kk_en_dev_synt)\n",
    "kk_en_train_kazparc = Dataset.from_pandas(kk_en_train_kazparc)\n",
    "kk_en_dev_kazparc = Dataset.from_pandas(kk_en_dev_kazparc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the synthetic and KazParC datasets\n",
    "\n",
    "kk_en_train_all = concatenate_datasets([kk_en_train_synt, kk_en_train_kazparc])\n",
    "kk_en_dev_all = concatenate_datasets([kk_en_dev_synt, kk_en_dev_kazparc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the tokenizer for KK->EN\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-1.3B\", src_lang='kaz_Cyrl', tgt_lang='eng_Latn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization for Kazakh to English \n",
    "\n",
    "kk_en_tokenized_data_train = kk_en_train_all.map(preprocessing, batched=True)\n",
    "kk_en_tokenized_data_dev = kk_en_dev_all.map(preprocessing, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the KK->EN and the vectorized datasets\n",
    "\n",
    "dataset_train = concatenate_datasets([dataset_train, kk_en_tokenized_data_train])\n",
    "dataset_dev = concatenate_datasets([dataset_dev, kk_en_tokenized_data_dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the tokenizer for EN->KK\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-1.3B\", src_lang='eng_Latn', tgt_lang='kaz_Cyrl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization for English to Kazakh\n",
    "\n",
    "kk_en_tokenized_data_train2 = kk_en_train_all.map(preprocessing_inverce, batched=True)\n",
    "kk_en_tokenized_data_dev2 = kk_en_dev_all.map(preprocessing_inverce, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the EN->KK and the vectorized datasets\n",
    "\n",
    "dataset_train = concatenate_datasets([dataset_train, kk_en_tokenized_data_train2])\n",
    "dataset_dev = concatenate_datasets([dataset_dev, kk_en_tokenized_data_dev2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synthetic KK-RU data\n",
    "\n",
    "kk_ru_train_synt = pd.read_csv('./data/19_sync_train_kk_ru.csv')\n",
    "kk_ru_dev_synt = pd.read_csv('./data/25_sync_valid_kk_ru.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load KazParC KK-RU data\n",
    "\n",
    "kk_ru_train_kazparc = pd.read_csv('./data/05_kazparc_train_kk_ru.csv')\n",
    "kk_ru_dev_kazparc = pd.read_csv('./data/11_kazparc_valid_kk_ru.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "\n",
    "kk_ru_train_kazparc.rename(columns={'kk': 'source_lang', 'ru': 'target_lang'}, inplace=True)\n",
    "kk_ru_dev_kazparc.rename(columns={'kk': 'source_lang', 'ru': 'target_lang'}, inplace=True)\n",
    "kk_ru_train_synt.rename(columns={'kk': 'source_lang', 'ru': 'target_lang'}, inplace=True)\n",
    "kk_ru_dev_synt.rename(columns={'kk': 'source_lang', 'ru': 'target_lang'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unused in training columns to reduse the datasets' size\n",
    "\n",
    "kk_ru_train_synt.drop(columns=['id'], inplace=True)\n",
    "kk_ru_dev_synt.drop(columns=['id'], inplace=True)\n",
    "kk_ru_train_kazparc.drop(columns=['id', 'domain'], inplace=True)\n",
    "kk_ru_dev_kazparc.drop(columns=['id', 'domain'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pandas datasets to HF format\n",
    "\n",
    "kk_ru_train_synt = Dataset.from_pandas(kk_ru_train_synt)\n",
    "kk_ru_dev_synt = Dataset.from_pandas(kk_ru_dev_synt)\n",
    "kk_ru_train_kazparc = Dataset.from_pandas(kk_ru_train_kazparc)\n",
    "kk_ru_dev_kazparc = Dataset.from_pandas(kk_ru_dev_kazparc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the synthetic and KazParC datasets\n",
    "\n",
    "kk_ru_train_all = concatenate_datasets([kk_ru_train_synt, kk_ru_train_kazparc])\n",
    "kk_ru_dev_all = concatenate_datasets([kk_ru_dev_synt, kk_ru_dev_kazparc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the tokenizer for KK->RU\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-1.3B\", src_lang='kaz_Cyrl', tgt_lang='rus_Cyrl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization for Kazakh to Russian \n",
    "\n",
    "kk_ru_tokenized_data_train = kk_ru_train_all.map(preprocessing, batched=True)\n",
    "kk_ru_tokenized_data_dev = kk_ru_dev_all.map(preprocessing, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the KK->RU and the vectorized datasets\n",
    "\n",
    "dataset_train = concatenate_datasets([dataset_train, kk_ru_tokenized_data_train])\n",
    "dataset_dev = concatenate_datasets([dataset_dev, kk_ru_tokenized_data_dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the tokenizer for RU->KK\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-1.3B\", src_lang='rus_Cyrl', tgt_lang='kaz_Cyrl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization for Russian to Kazakh\n",
    "\n",
    "kk_ru_tokenized_data_train2 = kk_ru_train_all.map(preprocessing_inverce, batched=True)\n",
    "kk_ru_tokenized_data_dev2 = kk_ru_dev_all.map(preprocessing_inverce, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the RU->KK and the vectorized datasets\n",
    "\n",
    "dataset_train = concatenate_datasets([dataset_train, kk_ru_tokenized_data_train2])\n",
    "dataset_dev = concatenate_datasets([dataset_dev, kk_ru_tokenized_data_dev2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synthetic KK-TR data\n",
    "\n",
    "kk_tr_train_synt = pd.read_csv('./data/20_sync_train_kk_tr.csv')\n",
    "kk_tr_dev_synt = pd.read_csv('./data/26_sync_valid_kk_tr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load KazParC KK-TR data\n",
    "\n",
    "kk_tr_train_kazparc = pd.read_csv('./data/06_kazparc_train_kk_tr.csv')\n",
    "kk_tr_dev_kazparc = pd.read_csv('./data/12_kazparc_valid_kk_tr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "\n",
    "kk_tr_train_kazparc.rename(columns={'kk': 'source_lang', 'tr': 'target_lang'}, inplace=True)\n",
    "kk_tr_dev_kazparc.rename(columns={'kk': 'source_lang', 'tr': 'target_lang'}, inplace=True)\n",
    "kk_tr_train_synt.rename(columns={'kk': 'source_lang', 'tr': 'target_lang'}, inplace=True)\n",
    "kk_tr_dev_synt.rename(columns={'kk': 'source_lang', 'tr': 'target_lang'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unused in training columns to reduse the datasets' size\n",
    "\n",
    "kk_tr_train_synt.drop(columns=['id'], inplace=True)\n",
    "kk_tr_dev_synt.drop(columns=['id'], inplace=True)\n",
    "kk_tr_train_kazparc.drop(columns=['id', 'domain'], inplace=True)\n",
    "kk_tr_dev_kazparc.drop(columns=['id', 'domain'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pandas datasets to HF format\n",
    "\n",
    "kk_tr_train_synt = Dataset.from_pandas(kk_tr_train_synt)\n",
    "kk_tr_dev_synt = Dataset.from_pandas(kk_tr_dev_synt)\n",
    "kk_tr_train_kazparc = Dataset.from_pandas(kk_tr_train_kazparc)\n",
    "kk_tr_dev_kazparc = Dataset.from_pandas(kk_tr_dev_kazparc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the synthetic and KazParC datasets\n",
    "\n",
    "kk_tr_train_all = concatenate_datasets([kk_tr_train_synt, kk_tr_train_kazparc])\n",
    "kk_tr_dev_all = concatenate_datasets([kk_tr_dev_synt, kk_tr_dev_kazparc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the tokenizer for KK->TR\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-1.3B\", src_lang='kaz_Cyrl', tgt_lang='tur_Latn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization for Kazakh to Turkish\n",
    "\n",
    "kk_tr_tokenized_data_train = kk_tr_train_all.map(preprocessing, batched=True)\n",
    "kk_tr_tokenized_data_dev = kk_tr_dev_all.map(preprocessing, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the KK->TR and the vectorized datasets\n",
    "\n",
    "dataset_train = concatenate_datasets([dataset_train, kk_tr_tokenized_data_train])\n",
    "dataset_dev = concatenate_datasets([dataset_dev, kk_tr_tokenized_data_dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the tokenizer for TR->KK\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-1.3B\", src_lang='tur_Latn', tgt_lang='kaz_Cyrl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization for Turkish to Kazakh\n",
    "\n",
    "kk_tr_tokenized_data_train2 = kk_tr_train_all.map(preprocessing_inverce, batched=True)\n",
    "kk_tr_tokenized_data_dev2 = kk_tr_dev_all.map(preprocessing_inverce, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the TR->KK and the vectorized datasets\n",
    "\n",
    "dataset_train = concatenate_datasets([dataset_train, kk_tr_tokenized_data_train2])\n",
    "dataset_dev = concatenate_datasets([dataset_dev, kk_tr_tokenized_data_dev2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synthetic RU-TR data\n",
    "\n",
    "ru_tr_train_synt = pd.read_csv('./data/21_sync_train_ru_tr.csv')\n",
    "ru_tr_dev_synt = pd.read_csv('./data/27_sync_valid_ru_tr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load KazParC RU-TR data\n",
    "\n",
    "ru_tr_train_kazparc = pd.read_csv('./data/07_kazparc_train_ru_tr.csv')\n",
    "ru_tr_dev_kazparc = pd.read_csv('./data/13_kazparc_valid_ru_tr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "\n",
    "ru_tr_train_kazparc.rename(columns={'ru': 'source_lang', 'tr': 'target_lang'}, inplace=True)\n",
    "ru_tr_dev_kazparc.rename(columns={'ru': 'source_lang', 'tr': 'target_lang'}, inplace=True)\n",
    "ru_tr_train_synt.rename(columns={'ru': 'source_lang', 'tr': 'target_lang'}, inplace=True)\n",
    "ru_tr_dev_synt.rename(columns={'ru': 'source_lang', 'tr': 'target_lang'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unused in training columns to reduse the datasets' size\n",
    "\n",
    "ru_tr_train_synt.drop(columns=['id'], inplace=True)\n",
    "ru_tr_dev_synt.drop(columns=['id'], inplace=True)\n",
    "ru_tr_train_kazparc.drop(columns=['id', 'domain'], inplace=True)\n",
    "ru_tr_dev_kazparc.drop(columns=['id', 'domain'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pandas datasets to HF format\n",
    "\n",
    "ru_tr_train_synt = Dataset.from_pandas(ru_tr_train_synt)\n",
    "ru_tr_dev_synt = Dataset.from_pandas(ru_tr_dev_synt)\n",
    "ru_tr_train_kazparc = Dataset.from_pandas(ru_tr_train_kazparc)\n",
    "ru_tr_dev_kazparc = Dataset.from_pandas(ru_tr_dev_kazparc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the synthetic and KazParC datasets\n",
    "\n",
    "ru_tr_train_all = concatenate_datasets([ru_tr_train_synt, ru_tr_train_kazparc])\n",
    "ru_tr_dev_all = concatenate_datasets([ru_tr_dev_synt, ru_tr_dev_kazparc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the tokenizer for RU->TR\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-1.3B\", src_lang='rus_Cyrl', tgt_lang='tur_Latn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization for Russian to Turkish\n",
    "\n",
    "ru_tr_tokenized_data_train = ru_tr_train_all.map(preprocessing, batched=True)\n",
    "ru_tr_tokenized_data_dev = ru_tr_dev_all.map(preprocessing, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the RU->TR and the vectorized datasets\n",
    "\n",
    "dataset_train = concatenate_datasets([dataset_train, ru_tr_tokenized_data_train])\n",
    "dataset_dev = concatenate_datasets([dataset_dev, ru_tr_tokenized_data_dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the tokenizer for TR->RU\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-1.3B\", src_lang='tur_Latn', tgt_lang='rus_Cyrl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization for Turkish to Russian\n",
    "\n",
    "ru_tr_tokenized_data_train2 = ru_tr_train_all.map(preprocessing_inverce, batched=True)\n",
    "ru_tr_tokenized_data_dev2 = ru_tr_dev_all.map(preprocessing_inverce, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the TR->RU and the vectorized datasets\n",
    "\n",
    "dataset_train = concatenate_datasets([dataset_train, ru_tr_tokenized_data_train2])\n",
    "dataset_dev = concatenate_datasets([dataset_dev, ru_tr_tokenized_data_dev2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train and dev datasets in local machine\n",
    "\n",
    "dataset_train.save_to_disk('./kazparc_train/')\n",
    "dataset_dev.save_to_disk('./kazparc_dev/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
